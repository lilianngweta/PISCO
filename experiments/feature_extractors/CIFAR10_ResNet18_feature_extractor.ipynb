{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as F\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from numpy import save\n",
    "\n",
    "from resnet_feature_extraction import Img2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download CIFAR 10 data form torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10('~/datasets/cifar', train=True, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10('~/datasets/cifar', train=False, download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_contrast(images_data, contrast_factor):\n",
    "    to_tensor = transforms.ToTensor()   \n",
    "    images_list = [to_tensor(im) for im in images_data]\n",
    "    images = torch.stack(images_list)\n",
    "    contrasted_images = F.adjust_contrast(images, contrast_factor)\n",
    "    contrasted_images = np.array(np.stack([transforms.ToPILImage()(im) for im in contrasted_images]))\n",
    "    return contrasted_images\n",
    "\n",
    "def apply_rotation(images_data, angle=30):\n",
    "    rotated_images = []\n",
    "    for img in images_data:\n",
    "        rotated_image = ndimage.rotate(img, angle, reshape=False)\n",
    "        rotated_images.append(rotated_image)\n",
    "    rotated_images = np.array(rotated_images)\n",
    "    return rotated_images\n",
    "\n",
    "def blur_images(images_data, sigma=1):\n",
    "    blurred_images = []\n",
    "    for img in images_data:\n",
    "        blurred_image = gaussian_filter(img, sigma)\n",
    "        blurred_images.append(blurred_image)\n",
    "    blurred_images = np.array(blurred_images)\n",
    "    return blurred_images\n",
    "\n",
    "\n",
    "def apply_saturation(images_data, sat_factor):\n",
    "    to_tensor = transforms.ToTensor()   \n",
    "    images_list = [to_tensor(im) for im in images_data]\n",
    "    images = torch.stack(images_list)\n",
    "    saturated_images = F.adjust_saturation(images, sat_factor)\n",
    "    saturated_images = np.array(np.stack([transforms.ToPILImage()(im) for im in saturated_images]))\n",
    "    return saturated_images\n",
    "\n",
    "\n",
    "# Function to extract image features using resnet    \n",
    "def get_features(images, batch_size):\n",
    "    Z_list = []\n",
    "    # img2vec = Img2Vec(model=\"resnet50\")\n",
    "    img2vec = Img2Vec()\n",
    "    for first in range(0, len(images), batch_size):\n",
    "        images_subset = images[first:first+batch_size]\n",
    "        Z_subset = img2vec.get_vec(images_subset)\n",
    "        Z_list.append(Z_subset)\n",
    "    Z = np.vstack(Z_list)\n",
    "    print(\"Z features shape\", Z.shape)\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform images and extract their features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform images \n",
    "\n",
    "# Transformation parameters\n",
    "angle = 15 # for rotation\n",
    "contrast_factor = 0.3 # for contrast\n",
    "sigma = 0.5 # for blur\n",
    "saturation_factor = 5 # for saturation\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_labels = np.array(train_dataset.targets)\n",
    "test_labels = np.array(test_dataset.targets)\n",
    "\n",
    "X_train_og = train_dataset.data\n",
    "X_train_rotated = apply_rotation(train_dataset.data, angle)\n",
    "X_train_contrasted = apply_contrast(train_dataset.data, contrast_factor) \n",
    "X_train_blurred = blur_images(train_dataset.data, sigma)\n",
    "X_train_saturated = apply_saturation(train_dataset.data, saturation_factor)\n",
    "\n",
    "\n",
    "X_test_og = test_dataset.data\n",
    "X_test_rotated = apply_rotation(test_dataset.data, angle)\n",
    "X_test_contrasted = apply_contrast(test_dataset.data, contrast_factor) \n",
    "X_test_blurred = blur_images(test_dataset.data, sigma)\n",
    "X_test_saturated = apply_saturation(test_dataset.data, saturation_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Extracting features using a pre-trained resnet 18 ###\n",
      "Z features shape (50000, 512)\n",
      "Z features shape (50000, 512)\n",
      "Z features shape (50000, 512)\n",
      "Z features shape (50000, 512)\n",
      "Z features shape (50000, 512)\n",
      "Z features shape (10000, 512)\n",
      "Z features shape (10000, 512)\n",
      "Z features shape (10000, 512)\n",
      "Z features shape (10000, 512)\n",
      "Z features shape (10000, 512)\n"
     ]
    }
   ],
   "source": [
    "# Extract image features \n",
    "print(\"### Extracting features using a pre-trained resnet 18 ###\")\n",
    "Z_train_og = get_features(X_train_og, batch_size)\n",
    "Z_train_rotated = get_features(X_train_rotated, batch_size)\n",
    "Z_train_contrasted = get_features(X_train_contrasted, batch_size)\n",
    "Z_train_blurred = get_features(X_train_blurred, batch_size)\n",
    "Z_train_saturated = get_features(X_train_saturated, batch_size)\n",
    "\n",
    "\n",
    "Z_test_og = get_features(X_test_og, batch_size )\n",
    "Z_test_rotated = get_features(X_test_rotated, batch_size)\n",
    "Z_test_contrasted = get_features(X_test_contrasted, batch_size)\n",
    "Z_test_blurred = get_features(X_test_blurred, batch_size)\n",
    "Z_test_saturated = get_features(X_test_saturated, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save extracted features and image labels as numpy files\n",
    "\n",
    "save('../data/Z_train_og_cifar10_resnet.npy', Z_train_og)\n",
    "save('../data/Z_train_rotated_cifar10_resnet.npy', Z_train_rotated)\n",
    "save('../data/Z_train_contrasted_cifar10_resnet.npy', Z_train_contrasted)\n",
    "save('../data/Z_train_blurred_cifar10_resnet.npy', Z_train_blurred)\n",
    "save('../data/Z_train_saturated_cifar10_resnet.npy', Z_train_saturated)\n",
    "\n",
    "save('../data/Z_test_og_cifar10_resnet.npy', Z_test_og)\n",
    "save('../data/Z_test_rotated_cifar10_resnet.npy', Z_test_rotated)\n",
    "save('../data/Z_test_contrasted_cifar10_resnet.npy', Z_test_contrasted)\n",
    "save('../data/Z_test_blurred_cifar10_resnet.npy', Z_test_blurred)\n",
    "save('../data/Z_test_saturated_cifar10_resnet.npy', Z_test_saturated)\n",
    "\n",
    "save('../data/train_labels_cifar10.npy', train_labels)\n",
    "save('../data/test_labels_cifar10.npy', test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:invariance_env]",
   "language": "python",
   "name": "conda-env-invariance_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
